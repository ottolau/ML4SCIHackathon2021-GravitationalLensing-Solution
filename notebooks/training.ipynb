{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7XXtAL-L-bE"
   },
   "source": [
    "Google colab libaray imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJcV78iWMBA8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/Colab\\ Notebooks/ML4SCI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ep0ONiRpflI6"
   },
   "source": [
    "Install libaray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJaqcBcqhpIU"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2KsrAQaMyt8"
   },
   "source": [
    "Import libaray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_V0eGAdXhDL"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf, re, math\n",
    "from pathlib import Path\n",
    "import math\n",
    "from tensorflow.keras import applications, layers, losses, optimizers, metrics, Model, backend\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.applications import ResNet50, EfficientNetB0, EfficientNetB3, EfficientNetB4, EfficientNetB6\n",
    "from tensorflow.keras import backend as K \n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import euclidean\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "import tqdm.notebook\n",
    "import gc\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWtLGwuSXu6u"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "BATCH_SIZE = 16\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeesUt7VviCq"
   },
   "source": [
    "define constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8p3nZ7kgYp2"
   },
   "outputs": [],
   "source": [
    "SEED = int(time.time())\n",
    "\n",
    "DEV = False\n",
    "# Training filenames directory\n",
    "if DEV:\n",
    "    TRAINING_FILENAME = 'tfrecord_train_shuffle.tfrec'\n",
    "    TRAINING_VAL_FILENAME = 'tfrecord_train_val_shuffle.tfrec'\n",
    "else:\n",
    "    TRAINING_FILENAME = 'tfrecord_train_full_shuffle.tfrec'\n",
    "    TRAINING_VAL_FILENAME = 'tfrecord_val.tfrec'\n",
    "\n",
    "TARGET_SHAPE = (150, 150)\n",
    "N_CLASSES = 3\n",
    "if DEV:\n",
    "    NUM_TRAINING_IMAGES = 24000\n",
    "else:\n",
    "    NUM_TRAINING_IMAGES = 30000\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3B2V7Ua0yKr"
   },
   "source": [
    "data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdJTl31ku3GL"
   },
   "outputs": [],
   "source": [
    "def random_apply(func, x, p):\n",
    "    \"\"\"Randomly apply function func to x with probability p.\"\"\"\n",
    "    return tf.cond(\n",
    "        tf.less(tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
    "                tf.cast(p, tf.float32)),\n",
    "        lambda: func(x),\n",
    "        lambda: x)\n",
    "\n",
    "def random_flip(image):\n",
    "    image = random_apply(tf.image.flip_left_right, image, p=0.5)\n",
    "    image = random_apply(tf.image.flip_up_down, image, p=0.5)\n",
    "    return image\n",
    "\n",
    "def random_rotate(image):\n",
    "    angle = tf.random.uniform([], minval=0, maxval=2.*math.pi, dtype=tf.float32)\n",
    "    image = tfa.image.rotate(image, angle)\n",
    "    return image\n",
    "\n",
    "def data_augmentation(image, label):\n",
    "    image = random_flip(image)\n",
    "    image = random_apply(random_rotate, image, 0.5)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9exvHKHGFAt"
   },
   "source": [
    "Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "eQGE4YYl72jI"
   },
   "outputs": [],
   "source": [
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature(list(TARGET_SHAPE), tf.float32),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = example['image']\n",
    "    # convert shape from (150, 150) to (150, 150, 3)\n",
    "    image = image * 255.0\n",
    "    image = tf.expand_dims(image, -1)\n",
    "    image = tf.image.grayscale_to_rgb(image)\n",
    "    label = tf.cast(example['label'], tf.int32)\n",
    "    label = tf.one_hot(label, N_CLASSES)\n",
    "    return image, label\n",
    "\n",
    "# This function loads TF Records and parse them into tensors\n",
    "def load_dataset(filenames):       \n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls = tf.data.experimental.AUTOTUNE) \n",
    "    return dataset\n",
    "\n",
    "# This function is to get our training tensors\n",
    "def get_training_dataset(filenames):\n",
    "    dataset = load_dataset(filenames)\n",
    "    dataset = dataset.map(lambda image, label: (image, label), num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.map(data_augmentation, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(1024)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(filenames):\n",
    "    dataset = load_dataset(filenames)\n",
    "    dataset = dataset.map(lambda image, label: (image, label), num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQf6vtHi8KUR"
   },
   "outputs": [],
   "source": [
    "train_dataset = get_training_dataset(TRAINING_FILENAME)\n",
    "train_val_dataset = get_validation_dataset(TRAINING_VAL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OD2gpr9ESrEp"
   },
   "source": [
    "Inspect training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "luOls6SK7WRs"
   },
   "outputs": [],
   "source": [
    "def batch_to_numpy_images_and_labels(data):\n",
    "    images, labels = data\n",
    "    numpy_images = images.numpy()\n",
    "    numpy_labels = labels.numpy()\n",
    "    return numpy_images, numpy_labels\n",
    "\n",
    "def display_one_image(image, title, subplot, red=False, titlesize=16):\n",
    "    plt.subplot(*subplot)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image[:,:,0]/255.0, cmap='gray')\n",
    "    plt.title(title, color='r')\n",
    "    return (subplot[0], subplot[1], subplot[2]+1)\n",
    "\n",
    "def display_batch_of_images(databatch, predictions=None):\n",
    "    # data\n",
    "    images, labels = batch_to_numpy_images_and_labels(databatch)\n",
    "        \n",
    "    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n",
    "    rows = int(math.sqrt(len(images)))\n",
    "    cols = len(images)//rows\n",
    "        \n",
    "    # size and spacing\n",
    "    FIGSIZE = 13.0\n",
    "    SPACING = 0.1\n",
    "    subplot=(rows,cols,1)\n",
    "    if rows < cols:\n",
    "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
    "    else:\n",
    "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
    "    \n",
    "    # display\n",
    "    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n",
    "        #if i > 20: break\n",
    "        title = label\n",
    "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n",
    "        subplot = display_one_image(image, title, subplot, titlesize=dynamic_titlesize)\n",
    "    \n",
    "    #layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.show()\n",
    "\n",
    "train_batch = train_dataset.unbatch().batch(20)\n",
    "train_batch = iter(train_batch)\n",
    "display_batch_of_images(next(train_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9EKi6_93wup"
   },
   "source": [
    "build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSDD7qu6ezD3"
   },
   "outputs": [],
   "source": [
    "class GeMPoolingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, p=1., train_p=False):\n",
    "        super().__init__()\n",
    "        if train_p:\n",
    "            self.p = tf.Variable(p, dtype=tf.float32)\n",
    "        else:\n",
    "            self.p = p\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    def call(self, inputs: tf.Tensor, **kwargs):\n",
    "        inputs = tf.clip_by_value(inputs, clip_value_min=1e-6, clip_value_max=tf.reduce_max(inputs))\n",
    "        inputs = tf.pow(inputs, self.p)\n",
    "        inputs = tf.reduce_mean(inputs, axis=[1, 2], keepdims=False)\n",
    "        inputs = tf.pow(inputs, 1./self.p)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbufs5Sh3uty"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    base_cnn = EfficientNetB3(\n",
    "        weights='imagenet', input_shape=TARGET_SHAPE + (3,), include_top=False, drop_connect_rate=0.4\n",
    "    )\n",
    "    outputs = GeMPoolingLayer(train_p=True)(base_cnn.output)\n",
    "    outputs = layers.Dense(N_CLASSES, activation=\"softmax\", name=\"pred\")(outputs)\n",
    "    model = Model(base_cnn.input, outputs, name=\"EfficientNet\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRxN8SlHVps1"
   },
   "source": [
    "LR Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHX--_ElV1hS"
   },
   "source": [
    "Cosine annealing learning rate scheduler with periodic restarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35k3-TsRU8nU"
   },
   "outputs": [],
   "source": [
    "class SGDRScheduler(tf.keras.callbacks.Callback):\n",
    "    '''Cosine annealing learning rate scheduler with periodic restarts.\n",
    "    # Usage\n",
    "        ```python\n",
    "            schedule = SGDRScheduler(min_lr=1e-5,\n",
    "                                     max_lr=1e-2,\n",
    "                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n",
    "                                     lr_decay=0.9,\n",
    "                                     cycle_length=5,\n",
    "                                     mult_factor=1.5)\n",
    "            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n",
    "        ```\n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        lr_decay: Reduce the max_lr after the completion of each cycle.\n",
    "                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n",
    "        cycle_length: Initial number of epochs in a cycle.\n",
    "        mult_factor: Scale epochs_to_restart after each full cycle completion.\n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: http://arxiv.org/abs/1608.03983\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 min_lr,\n",
    "                 max_lr,\n",
    "                 steps_per_epoch,\n",
    "                 lr_decay=1,\n",
    "                 cycle_length=10,\n",
    "                 mult_factor=2):\n",
    "\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.batch_since_restart = 0\n",
    "        self.next_restart = cycle_length\n",
    "\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        self.cycle_length = cycle_length\n",
    "        self.mult_factor = mult_factor\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n",
    "        return lr\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.max_lr)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        self.batch_since_restart += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''Check for end of current cycle, apply restarts when necessary.'''\n",
    "        if epoch + 1 == self.next_restart:\n",
    "            self.batch_since_restart = 0\n",
    "            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n",
    "            self.next_restart += self.cycle_length\n",
    "            self.max_lr *= self.lr_decay\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjkmGjeRjDj5"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3fZFQjj9t85"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class PlotLearning(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback to plot the learning curves of the model during training.\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "            \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Storing metrics\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "        \n",
    "        # Plotting\n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "        \n",
    "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axs[i].plot(range(1, epoch + 2), \n",
    "                        self.metrics[metric], \n",
    "                        label=metric)\n",
    "            if logs['val_' + metric]:\n",
    "                axs[i].plot(range(1, epoch + 2), \n",
    "                            self.metrics['val_' + metric], \n",
    "                            label='val_' + metric)\n",
    "                \n",
    "            axs[i].legend()\n",
    "            axs[i].grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFSzQtqXVH56"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"weights_effnetb3_final.{epoch:05d}.hdf5\"\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 monitor = 'val_loss',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=False,\n",
    "                                                 mode = 'min',\n",
    "                                                 verbose=1)\n",
    "\n",
    "num_epochs = 100\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE #// 10\n",
    "\n",
    "# cosine annealing learning rate scheduler with periodic restarts\n",
    "lr_sched = SGDRScheduler(min_lr=1.e-6,\n",
    "                        max_lr=5.e-4,\n",
    "                        steps_per_epoch=np.ceil(NUM_TRAINING_IMAGES / BATCH_SIZE),\n",
    "                        lr_decay=0.85,\n",
    "                        cycle_length=20,\n",
    "                        mult_factor=1.5)\n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=1.e-4, weight_decay=1e-5, clipvalue=700)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "with strategy.scope():\n",
    "    model = get_model()\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(\n",
    "        loss = loss,\n",
    "        metrics = [tf.keras.metrics.CategoricalAccuracy(),\n",
    "                   ],\n",
    "        optimizer = optimizer,  \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9K6MfMjiSmGB"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    history = model.fit(\n",
    "        train_dataset, steps_per_epoch=STEPS_PER_EPOCH, epochs=num_epochs, validation_data=train_val_dataset, callbacks=[cp_callback, lr_sched, PlotLearning()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cT34rQ4Jc70t"
   },
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "# or save to csv: \n",
    "hist_csv_file = 'history_effnetb3_final.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFA1neB8Q2tx"
   },
   "source": [
    "Testing the CNN Model on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-nOa9uIjKQ6"
   },
   "outputs": [],
   "source": [
    "VALIDATION_FILENAME = 'tfrecord_val.tfrec'\n",
    "val_dataset = get_validation_dataset(VALIDATION_FILENAME)\n",
    "\n",
    "with strategy.scope():\n",
    "    trained_model = get_model()\n",
    "    trained_model.load_weights('weights_effnetb3_final.00089.hdf5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3AiGcEaZRsP"
   },
   "source": [
    "Get the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrzQFnh8SzTb"
   },
   "outputs": [],
   "source": [
    "# Get the true label in one-hot e.g. [[0, 1, 0], [0, 0, 1], ...]\n",
    "y_val = np.concatenate([y for x, y in val_dataset], axis=0)\n",
    "\n",
    "# Get the prodicted label e.g. [[0.999, 0.001, 0.01], [0.023, 0.982, 0.001], ...]\n",
    "with strategy.scope():\n",
    "    y_score = trained_model.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFi3_oFdZUMp"
   },
   "source": [
    "Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lkhmHODU8Gm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "n_classes = y_val.shape[1]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_val[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_val.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [7, 5]\n",
    "lw = 2\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average (area = {})'\n",
    "               ''.format(round(roc_auc[\"micro\"],5)),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average (area = {})'\n",
    "               ''.format(round(roc_auc[\"macro\"],5)),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "labels = ['no sub', 'spherical', 'vortex']\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='{} (area = {})'\n",
    "             ''.format(labels[i], round(roc_auc[i],5)))\n",
    "\n",
    "# Plot the ROC \n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\", prop={\"size\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePa19tjZY2F2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "submission_ml4sci_graviational_lensing_classification_efficientnet_gpu.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
